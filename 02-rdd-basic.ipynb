{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "# 02 - RDD Basics\n",
    "\n",
    "This notebooks describes the concept of RDD and operations on RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Startup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check if spark is running (don't forget there is a **Spark UI** available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.31.71.25:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fccf00210f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Resilient Distributed Datasets (RDD)\n",
    "\n",
    "* immutable (read-only), distributed and resilient (fault-tolerant)\n",
    "* data is distributed accross nodes\n",
    "\n",
    "The RDD PySpark API is [here](http://spark.apache.org/docs/default/api/python/pyspark.html#pyspark.RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[43] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RDD can contains several entity with different types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([\"abc\",1,2.34,False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access all elements in the RDD using [`collect()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) (**WARNING:** the content of all the RDD is returned to the Spark driver, *i.e.* the python kernel of your notebook in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', 1, 2.34, False]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the first element of the RDD using [`first()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the first elements of the RDD using [`take()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take) (**SAME WARNING AS PREVIOUS CELL**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', 1, 2.34]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## RDD transformations and actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD Computation are **lazy**: they are launched only when needed\n",
    "* [`parallelize()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.parallelize) is a **transformation**, it produces a new RDD\n",
    "* [`count()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.count) is an **action**, it gives non-RDD values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[51] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations build an **RDD lineage** (also known as RDD operator graph or RDD dependency graph), which is a logical execution plan, *i.e.* a **Directed Acyclic Graph** (DAG) of the entire parent RDDs of RDD. An action is the final operation of a RDD lineage and it starts all RDD transformations following the DAG to obtain the output value of the action.\n",
    "\n",
    "For instance, the following cell contains 3 transformations, and the action is [`collect()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(range(10)).map(lambda x: x * 2).filter(lambda x: x > 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[54] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Transformations\n",
    "\n",
    "Let's see some transformation methods, you can guess by yourself what they are doing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [`map()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(10)).map(lambda x: x + 2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [`flatMap()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 1, 3, 2, 4, 3, 5, 4, 6, 5, 7, 6, 8, 7, 9, 8, 10, 9, 11]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(10)).flatMap(lambda x: (x, x + 2)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [`filter()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(['a','b','c','d','e']).filter(lambda x: x != 'a').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [`union()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e', 'd', 'e', 'f', 'g', 'h']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(['a','b','c','d','e'])\n",
    "rdd2 = sc.parallelize(['d','e','f','g','h'])\n",
    "rdd1.union(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [`intersection()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 'd']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(['a','b','c','d','e'])\n",
    "rdd2 = sc.parallelize(['d','e','f','g','h'])\n",
    "rdd1.intersection(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD actions\n",
    "\n",
    "Actions are the final step after a set of transformations, and returns a non-RDD results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([4, 8, 3, 1, 7, 6, 5, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 8, 3, 1, 7, 6, 5, 2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.29128784747792"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.25"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 8, 3]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.takeOrdered(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 7, 6, 4, 4, 8]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.takeSample(num=6,withReplacement=True,seed=0xCAFEBABE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 2, 8, 4, 1, 3]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.takeSample(num=6,withReplacement=False,seed=0xCAFEBABE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Pair RDD\n",
    "\n",
    "Pair RDD is a RDD containing key/values pairs, *i.e.* tuple of data. Pair RDD transformations produce key,value pairs whereas operations on RDD gives you a collection of values or a single value\n",
    "\n",
    "*Note:* Some examples are stolen from this [tutorial](https://programmathics.com/big-data/apache-spark/apache-spark-working-with-key-value-pairs-in-rdd/apache-spark-creating-pair-rdds-and-associated-transformations-part-1/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1= sc.parallelize([(1,2),(3,2),(2,4),(1,2),(3,4),(1,5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`keys()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 2, 1, 3, 1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`values()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 4, 2, 4, 5]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 3, 3: 2, 2: 1})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {(1, 2): 2, (3, 2): 1, (2, 4): 1, (3, 4): 1, (1, 5): 1})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 3), (4, 3), (3, 5), (2, 3), (4, 5), (2, 6)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.map(lambda x: (x[0]+1,x[1]+1)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`mapValues()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mapValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 3, 4)),\n",
       " (3, (2, 3, 4)),\n",
       " (2, (4, 3, 4)),\n",
       " (1, (2, 3, 4)),\n",
       " (3, (4, 3, 4)),\n",
       " (1, (5, 3, 4))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(1, [10, 11, 12]), (2, [20, 21])])\n",
    "rdd1.mapValues(lambda x: (x, 3,4)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`flatMapValues()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMapValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, [10, 11, 12]), (1, 3), (1, 4), (2, [20, 21]), (2, 3), (2, 4)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(1, [10, 11, 12]), (2, [20, 21])])\n",
    "rdd.flatMapValues(lambda x: (x, 3,4)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** sortByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Joins operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([(1,\"A\"),(2,\"B\"),(3,\"C\"),(4,\"D\"),(5,\"E\")])\n",
    "rdd2 = sc.parallelize([(4,\"W\"),(5,\"X\"),(6,\"Y\"),(7,\"Z\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ('A', None)),\n",
       " (2, ('B', None)),\n",
       " (3, ('C', None)),\n",
       " (4, ('D', 'W')),\n",
       " (5, ('E', 'X'))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.leftOuterJoin(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'A'), (2, 'B'), (3, 'C')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.subtractByKey(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, ('D', 'W')), (5, ('E', 'X'))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.join(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, ('D', 'W')), (5, ('E', 'X')), (6, (None, 'Y')), (7, (None, 'Z'))]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.rightOuterJoin(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [`cartesian()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.cartesian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 7),\n",
       " (0, 8),\n",
       " (1, 7),\n",
       " (1, 8),\n",
       " (0, 9),\n",
       " (0, 10),\n",
       " (0, 11),\n",
       " (1, 9),\n",
       " (1, 10),\n",
       " (1, 11)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(range(10))\n",
    "rdd2 = sc.parallelize(range(7,17))\n",
    "rdd1.cartesian(rdd2).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## [`reduce()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduce)\n",
    "\n",
    "action that aggregates all the elements of the RDD using using the specified commutative and associative binary operatorand returns the final result to the driver program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,2,5])\n",
    "rdd.reduce(lambda a,b : a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduce(lambda a,b : a + (1 if b % 2 else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 2], 3], [2, 5]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduce(lambda a,b : [a,b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Group operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### [`reduceByKey()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey)\n",
    "\n",
    "Same as reduce() but grouping by key. For instance let's do a Map-Reduce operation (Hadoop-like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(\"\"\"i decided, very early on, just to accept life unconditionally; i never expected it to do anything special for me, yet i seemed\n",
    "to accomplish far more than i had ever hoped. most of the time it just happened to me without me ever seeking it.\"\"\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'decided,',\n",
       " 'very',\n",
       " 'early',\n",
       " 'on,',\n",
       " 'just',\n",
       " 'to',\n",
       " 'accept',\n",
       " 'life',\n",
       " 'unconditionally;']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = data.map(lambda s: (s, 1)).reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 4),\n",
       " ('yet', 1),\n",
       " ('seemed', 1),\n",
       " ('far', 1),\n",
       " ('ever', 2),\n",
       " ('of', 1),\n",
       " ('early', 1),\n",
       " ('accept', 1),\n",
       " ('life', 1),\n",
       " ('me,', 1),\n",
       " ('accomplish', 1),\n",
       " ('without', 1),\n",
       " ('decided,', 1),\n",
       " ('on,', 1),\n",
       " ('just', 2),\n",
       " ('unconditionally;', 1),\n",
       " ('anything', 1),\n",
       " ('than', 1),\n",
       " ('it', 2),\n",
       " ('for', 1),\n",
       " ('hoped.', 1),\n",
       " ('do', 1),\n",
       " ('happened', 1),\n",
       " ('to', 4),\n",
       " ('special', 1),\n",
       " ('most', 1),\n",
       " ('the', 1),\n",
       " ('time', 1),\n",
       " ('seeking', 1),\n",
       " ('very', 1),\n",
       " ('never', 1),\n",
       " ('more', 1),\n",
       " ('it.', 1),\n",
       " ('expected', 1),\n",
       " ('had', 1),\n",
       " ('me', 2)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`groupByKey()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey)\n",
    " \n",
    "It's aggregate type method with naive map-reduce approach: all keys are sent to reducers with high network I/O (shuffle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 4),\n",
       " ('do', 1),\n",
       " ('yet', 1),\n",
       " ('seemed', 1),\n",
       " ('far', 1),\n",
       " ('ever', 2),\n",
       " ('of', 1),\n",
       " ('happened', 1),\n",
       " ('early', 1),\n",
       " ('to', 4),\n",
       " ('accept', 1),\n",
       " ('life', 1),\n",
       " ('special', 1),\n",
       " ('me,', 1),\n",
       " ('accomplish', 1),\n",
       " ('most', 1),\n",
       " ('the', 1),\n",
       " ('time', 1),\n",
       " ('without', 1),\n",
       " ('seeking', 1),\n",
       " ('decided,', 1),\n",
       " ('very', 1),\n",
       " ('on,', 1),\n",
       " ('just', 2),\n",
       " ('unconditionally;', 1),\n",
       " ('never', 1),\n",
       " ('anything', 1),\n",
       " ('more', 1),\n",
       " ('than', 1),\n",
       " ('it.', 1),\n",
       " ('expected', 1),\n",
       " ('it', 2),\n",
       " ('for', 1),\n",
       " ('had', 1),\n",
       " ('hoped.', 1),\n",
       " ('me', 2)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupBy(lambda x : x).mapValues(lambda x: len(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`combineByKey()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.combineByKey)\n",
    "\n",
    "Variant of reduceByKey with three parameters and the notion of combiner. \n",
    "\n",
    "* first: create the combiner (init the combiner, usually with a one-element list)\n",
    "* second: merge the values of the combiners\n",
    "* third: merge the combiners\n",
    "\n",
    "The following example is stolen from pyspark documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1, 2]), ('b', [1])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
    "\n",
    "# First\n",
    "def to_list(a):\n",
    "    return [a]\n",
    "\n",
    "# Second\n",
    "def append(a, b):\n",
    "    a.append(b)\n",
    "    return a\n",
    "\n",
    "# Third\n",
    "def extend(a, b):\n",
    "    a.extend(b)\n",
    "    return a\n",
    "\n",
    "sorted(rdd.combineByKey(to_list, append, extend).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 4),\n",
       " ('do', 1),\n",
       " ('yet', 1),\n",
       " ('seemed', 1),\n",
       " ('far', 1),\n",
       " ('ever', 2),\n",
       " ('of', 1),\n",
       " ('happened', 1),\n",
       " ('early', 1),\n",
       " ('to', 4),\n",
       " ('accept', 1),\n",
       " ('life', 1),\n",
       " ('special', 1),\n",
       " ('me,', 1),\n",
       " ('accomplish', 1),\n",
       " ('most', 1),\n",
       " ('the', 1),\n",
       " ('time', 1),\n",
       " ('without', 1),\n",
       " ('seeking', 1),\n",
       " ('decided,', 1),\n",
       " ('very', 1),\n",
       " ('on,', 1),\n",
       " ('just', 2),\n",
       " ('unconditionally;', 1),\n",
       " ('never', 1),\n",
       " ('anything', 1),\n",
       " ('more', 1),\n",
       " ('than', 1),\n",
       " ('it.', 1),\n",
       " ('expected', 1),\n",
       " ('it', 2),\n",
       " ('for', 1),\n",
       " ('had', 1),\n",
       " ('hoped.', 1),\n",
       " ('me', 2)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_tuple(x):\n",
    "    return (x,1)\n",
    "\n",
    "def identity(a):\n",
    "    return a\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "data.map(create_tuple).combineByKey(identity, add, add).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same with lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 4),\n",
       " ('do', 1),\n",
       " ('yet', 1),\n",
       " ('seemed', 1),\n",
       " ('far', 1),\n",
       " ('ever', 2),\n",
       " ('of', 1),\n",
       " ('happened', 1),\n",
       " ('early', 1),\n",
       " ('to', 4),\n",
       " ('accept', 1),\n",
       " ('life', 1),\n",
       " ('special', 1),\n",
       " ('me,', 1),\n",
       " ('accomplish', 1),\n",
       " ('most', 1),\n",
       " ('the', 1),\n",
       " ('time', 1),\n",
       " ('without', 1),\n",
       " ('seeking', 1),\n",
       " ('decided,', 1),\n",
       " ('very', 1),\n",
       " ('on,', 1),\n",
       " ('just', 2),\n",
       " ('unconditionally;', 1),\n",
       " ('never', 1),\n",
       " ('anything', 1),\n",
       " ('more', 1),\n",
       " ('than', 1),\n",
       " ('it.', 1),\n",
       " ('expected', 1),\n",
       " ('it', 2),\n",
       " ('for', 1),\n",
       " ('had', 1),\n",
       " ('hoped.', 1),\n",
       " ('me', 2)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(lambda x: (x,1)).combineByKey(\\\n",
    "    lambda value: value,\\\n",
    "    lambda x, value: x + value,\\\n",
    "    lambda x, y: x+y)\\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: `fold()`, `foldByKey()`, `aggregate()` and `aggregateByKey()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "passwd_rdd = sc.textFile('/etc/passwd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passwd_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "passwd_uid = passwd_rdd.map(lambda x: x.split(':')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "passwd_gid = passwd_rdd.map(lambda x: x.split(':')[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '65534',\n",
       " '60',\n",
       " '12',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '13',\n",
       " '33',\n",
       " '34',\n",
       " '38',\n",
       " '39',\n",
       " '41',\n",
       " '65534',\n",
       " '102',\n",
       " '103',\n",
       " '106',\n",
       " '107',\n",
       " '65534',\n",
       " '111',\n",
       " '112',\n",
       " '46',\n",
       " '65534',\n",
       " '114',\n",
       " '29',\n",
       " '117',\n",
       " '65534',\n",
       " '119',\n",
       " '120',\n",
       " '122',\n",
       " '123',\n",
       " '7',\n",
       " '124',\n",
       " '65534',\n",
       " '125',\n",
       " '100',\n",
       " '116',\n",
       " '128',\n",
       " '65534',\n",
       " '130',\n",
       " '133',\n",
       " '65534']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passwd_gid.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
